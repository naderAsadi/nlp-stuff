{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Seq2Seq.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPK2FZGEN2TU4CVbQsrCRas"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"BHSs58qNKEbz","colab_type":"code","colab":{}},"source":["!pip install torchtext==0.5.0\n","!pip install -U spacy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZbqSDwn6LBL4","colab_type":"code","colab":{}},"source":["!python -m spacy download en\n","!python -m spacy download de"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"caOftfkh_Uc-","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","from torchtext import data\n","from torchtext.datasets import Multi30k"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UZqfAMTkJm5g","colab_type":"text"},"source":["## Data"]},{"cell_type":"code","metadata":{"id":"9auwijL3H6xS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"outputId":"4e13f087-c018-45c4-ded1-c799728a7d59","executionInfo":{"status":"ok","timestamp":1584768100009,"user_tz":-270,"elapsed":38885,"user":{"displayName":"Nader Asadi","photoUrl":"","userId":"13987168146830248957"}}},"source":["SRC = data.Field(tokenize = \"spacy\",\n","            tokenizer_language=\"de\",\n","            init_token = '<sos>',\n","            eos_token = '<eos>',\n","            lower = True)\n","\n","TRG = data.Field(tokenize = \"spacy\",\n","            tokenizer_language=\"en\",\n","            init_token = '<sos>',\n","            eos_token = '<eos>',\n","            lower = True)\n","\n","train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'),\n","                                                    fields=(SRC, TRG))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["\rtraining.tar.gz:   0%|          | 0.00/1.21M [00:00<?, ?B/s]"],"name":"stderr"},{"output_type":"stream","text":["downloading training.tar.gz\n"],"name":"stdout"},{"output_type":"stream","text":["training.tar.gz: 100%|██████████| 1.21M/1.21M [00:00<00:00, 4.42MB/s]\n","validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 1.52MB/s]"],"name":"stderr"},{"output_type":"stream","text":["downloading validation.tar.gz\n","downloading mmt_task1_test2016.tar.gz\n"],"name":"stdout"},{"output_type":"stream","text":["\n","mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 1.40MB/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"NVIhUtfkMI1c","colab_type":"text"},"source":["### Build Vocabularies"]},{"cell_type":"markdown","metadata":{"id":"edLswN0FMPbV","colab_type":"text"},"source":["We can see an extremely useful feature of torchtext’s Field: the <b>build_vocab</b> method now allows us to create the vocabulary associated with each language."]},{"cell_type":"code","metadata":{"id":"GaH2dg6VH6z_","colab_type":"code","colab":{}},"source":["SRC.build_vocab(train_data, min_freq=2)\n","TRG.build_vocab(train_data, min_freq=2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6tzpPjzcNBHd","colab_type":"text"},"source":["Once these lines of code have been run, <b>SRC.vocab.stoi</b> will be a dictionary with the tokens in the vocabulary as keys and their corresponding indices as values; <b>SRC.vocab.itos</b> will be the same dictionary with the keys and values swapped. We won’t make extensive use of this fact in this tutorial, but this will likely be useful in other NLP tasks you’ll encounter."]},{"cell_type":"markdown","metadata":{"id":"xI_1Ie2rNQTo","colab_type":"text"},"source":["<hr>\n","<b>BucketIterator</b>: Defines an iterator that batches examples of similar lengths together. Minimizes amount of padding needed while producing freshly shuffled batches for each new epoch. See pool for the bucketing procedure used."]},{"cell_type":"code","metadata":{"id":"Eybqj_kVH62G","colab_type":"code","colab":{}},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","BATCH_SIZE = 128\n","\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","    (train_data, valid_data, test_data),\n","    batch_size = BATCH_SIZE,\n","    device = device\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yIMOCOVqWDqz","colab_type":"text"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"13-tZRofWDNY","colab_type":"code","colab":{}},"source":["import random\n","from typing import Tuple\n","\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch import Tensor"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tNS7ngkHWDP5","colab_type":"code","colab":{}},"source":["class Encoder(nn.Module):\n","    def __init__(self, input_dim: int, emb_dim: int, enc_hid_dim: int, dec_hid_dim: int,\n","                 dropout: float):\n","        super().__init__()\n","\n","        self.input_dim = input_dim\n","        self.emb_dim = emb_dim\n","        self.enc_hid_dim = enc_hid_dim\n","        self.dec_hid_dim = dec_hid_dim\n","        self.dropout = dropout\n","\n","        self.embedding = nn.Embedding(input_dim, emb_dim)\n","\n","        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n","        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, src: Tensor) -> Tuple[Tensor]:\n","        embedded = self.dropout(self.embedding(src))\n","        outputs, hidden = self.rnn(embedded)\n","        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n","\n","        return outputs, hidden"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VhOYr08dWDLV","colab_type":"code","colab":{}},"source":["class Attention(nn.Module):\n","    def __init__(self, enc_hid_dim: int, dec_hid_dim: int, attn_dim: int):\n","        super().__init__()\n","\n","        self.enc_hid_dim = enc_hid_dim\n","        self.dec_hid_dim = dec_hid_dim\n","\n","        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n","        self.attn = nn.Linear(self.attn_in, attn_dim)\n","\n","    def forward(self, decoder_hidden: Tensor, encoder_outputs: Tensor) -> Tensor:\n","        src_len = encoder_outputs.shape[0]\n","\n","        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n","\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n","\n","        energy = torch.tanh(self.attn(torch.cat((\n","            repeated_decoder_hidden,\n","            encoder_outputs),\n","            dim = 2)))\n","\n","        attention = torch.sum(energy, dim=2)\n","        return F.softmax(attention, dim=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BXtxVUdsH64f","colab_type":"code","colab":{}},"source":["class Decoder(nn.Module):\n","    def __init__(self, output_dim: int, emb_dim: int, enc_hid_dim: int, dec_hid_dim: int,\n","                 dropout: int, attention: nn.Module):\n","        super().__init__()\n","\n","        self.emb_dim = emb_dim\n","        self.enc_hid_dim = enc_hid_dim\n","        self.dec_hid_dim = dec_hid_dim\n","        self.output_dim = output_dim\n","        self.dropout = dropout\n","        self.attention = attention\n","\n","        self.embedding = nn.Embedding(output_dim, emb_dim)\n","\n","        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n","        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","\n","    def _weighted_encoder_rep(self,decoder_hidden: Tensor, encoder_outputs: Tensor) -> Tensor:\n","\n","        a = self.attention(decoder_hidden, encoder_outputs)\n","        a = a.unsqueeze(1)\n","\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n","        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n","        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n","\n","        return weighted_encoder_rep\n","\n","\n","    def forward(self, input: Tensor, decoder_hidden: Tensor, encoder_outputs: Tensor) -> Tuple[Tensor]:\n","\n","        input = input.unsqueeze(0)\n","        embedded = self.dropout(self.embedding(input))\n","        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,\n","                                                          encoder_outputs)\n","\n","        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n","        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n","\n","        embedded = embedded.squeeze(0)\n","        output = output.squeeze(0)\n","        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n","\n","        output = self.out(torch.cat((output,\n","                                     weighted_encoder_rep,\n","                                     embedded), dim = 1))\n","\n","        return output, decoder_hidden.squeeze(0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v3EKXxX4ZL0t","colab_type":"code","colab":{}},"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder: nn.Module, decoder: nn.Module, device: torch.device):\n","        super().__init__()\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","    def forward(self, src: Tensor, trg: Tensor, teacher_forcing_ratio: float = 0.5) -> Tensor:\n","        batch_size = src.shape[1]\n","        max_len = trg.shape[0]\n","        trg_vocab_size = self.decoder.output_dim\n","\n","        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n","        encoder_outputs, hidden = self.encoder(src)\n","\n","        # first input to the decoder is the <sos> token\n","        output = trg[0,:]\n","\n","        for t in range(1, max_len):\n","            output, hidden = self.decoder(output, hidden, encoder_outputs)\n","            outputs[t] = output\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            top1 = output.max(1)[1]\n","            output = (trg[t] if teacher_force else top1)\n","\n","        return outputs"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jAHa6bTXbW22","colab_type":"text"},"source":["## Training and Evaluation"]},{"cell_type":"code","metadata":{"id":"kRO-6eXiZL3c","colab_type":"code","colab":{}},"source":["INPUT_DIM = len(SRC.vocab)\n","OUTPUT_DIM = len(TRG.vocab)\n","\n","ENC_EMB_DIM = 32\n","DEC_EMB_DIM = 32\n","ENC_HID_DIM = 64\n","DEC_HID_DIM = 64\n","ATTN_DIM = 8\n","ENC_DROPOUT = 0.5\n","DEC_DROPOUT = 0.5"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sZ3LzS0xZL6B","colab_type":"code","colab":{}},"source":["enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n","\n","attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n","\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n","\n","model = Seq2Seq(enc, dec, device).to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4A8SriFoZL8c","colab_type":"code","colab":{}},"source":["def init_weights(m: nn.Module):\n","    for name, param in m.named_parameters():\n","        if 'weight' in name:\n","            nn.init.normal_(param.data, mean=0, std=0.01)\n","        else:\n","            nn.init.constant_(param.data, 0)\n","\n","\n","model.apply(init_weights)\n","\n","optimizer = optim.Adam(model.parameters())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XPGTdkVOZLyi","colab_type":"code","outputId":"31a816bb-6602-4205-cd2a-bc4e788a161b","executionInfo":{"status":"ok","timestamp":1584768353583,"user_tz":-270,"elapsed":1247,"user":{"displayName":"Nader Asadi","photoUrl":"","userId":"13987168146830248957"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["def count_parameters(model: nn.Module):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":35,"outputs":[{"output_type":"stream","text":["The model has 1,856,685 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lycqsGoBbwL-","colab_type":"text"},"source":["<hr>\n","Note: when scoring the performance of a language translation model in particular, we have to tell the <b>nn.CrossEntropyLoss</b> function to <b>ignore the indices where the target is simply padding."]},{"cell_type":"code","metadata":{"id":"nn_qfPHqbjiy","colab_type":"code","colab":{}},"source":["PAD_IDX = TRG.vocab.stoi['<pad>']\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j_Ui7feNbrBQ","colab_type":"code","colab":{}},"source":["import math\n","import time\n","\n","def train(model: nn.Module, iterator: data.BucketIterator, optimizer: optim.Optimizer, criterion: nn.Module,\n","          clip: float):\n","    model.train()\n","    epoch_loss = 0\n","\n","    for _, batch in enumerate(iterator):\n","        src = batch.src\n","        trg = batch.trg\n","\n","        optimizer.zero_grad()\n","        output = model(src, trg)\n","        output = output[1:].view(-1, output.shape[-1])\n","        trg = trg[1:].view(-1)\n","\n","        loss = criterion(output, trg)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)\n","\n","\n","def evaluate(model: nn.Module, iterator: data.BucketIterator, criterion: nn.Module):\n","    model.eval()\n","    epoch_loss = 0\n","    with torch.no_grad():\n","        for _, batch in enumerate(iterator):\n","            src = batch.src\n","            trg = batch.trg\n","\n","            output = model(src, trg, 0) #turn off teacher forcing\n","            output = output[1:].view(-1, output.shape[-1])\n","            trg = trg[1:].view(-1)\n","\n","            loss = criterion(output, trg)\n","            epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)\n","\n","def epoch_time(start_time: int, end_time: int):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DOYOLJwvcsj-","colab_type":"code","colab":{}},"source":["N_EPOCHS = 10\n","CLIP = 1\n","best_valid_loss = float('inf')\n","for epoch in range(N_EPOCHS):\n","    start_time = time.time()\n","\n","    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n","    valid_loss = evaluate(model, valid_iterator, criterion)\n","\n","    end_time = time.time()\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n","\n","test_loss = evaluate(model, test_iterator, criterion)\n","\n","print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EtEmKBwTc7gy","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}